{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYAM2RR5s6Po",
    "outputId": "b1545a98-b362-4901-c704-17c5e2053cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ephi7w6DC1k"
   },
   "source": [
    "# **Kütüphaneler**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xobG9ip4s7JI",
    "outputId": "e83cdb88-7514-4c4c-b591-edf7838dce4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Text Preprocessor Libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "# Word Embedding Modeli\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Model performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BERT libraries\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# LSTM libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# CNN libraries\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "# Class Weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    '''\n",
    "    Türkçe metin ön işleme sınıfı.\n",
    "    Bu sınıf, Türkçe metinler üzerinde çeşitli ön işleme adımları uygular:\n",
    "    - Noktalama işaretlerini kaldırma\n",
    "    - Sayıları kaldırma\n",
    "    - Özel karakterleri temizleme\n",
    "    - Stopword'leri kaldırma\n",
    "    - Kelimeleri köklerine ayırma\n",
    "    - Kısa kelimeleri filtreleme\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Türkçe stopword'leri yükle\n",
    "        self.stop_words = set(stopwords.words('turkish'))\n",
    "        # Zemberek kütüphanesi ile Türkçe morfolojik analiz oluştur\n",
    "        self.morphology = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "    def find_root(self, word):\n",
    "        '''Bir kelimenin kökünü bulur. Eğer analiz sonucu yoksa orijinal kelimeyi döndürür.'''\n",
    "        results = self.morphology.analyze(word)\n",
    "        for result in results:\n",
    "            return result.get_s85tem()\n",
    "        return word\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        '''Metindeki tüm kelimelerin köklerini çıkarır.'''\n",
    "        words = text.split()\n",
    "        lemmatized_words = [self.find_root(word) for word in words]\n",
    "        return \" \".join(lemmatized_words)\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        '''Noktalama işaretlerini metinden kaldırır.'''\n",
    "        return re.sub(r'[\\W]', ' ', text)\n",
    "\n",
    "    def remove_numbers(self, text):\n",
    "        '''Metindeki sayıları kaldırır.'''\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        '''Özel karakterleri metinden kaldırır.'''\n",
    "        return re.sub(r'_x000D_', '', text)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        '''Türkçe stopword'leri metinden kaldırır.'''\n",
    "        return \" \".join(word.lower() for word in text.split() if word.lower() not in self.stop_words)\n",
    "\n",
    "    def remove_short_words(self, text):\n",
    "        '''2 karakterden kısa kelimeleri metinden kaldırır.'''\n",
    "        return \" \".join(word for word in text.split() if len(word) > 2)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        '''\n",
    "        Tüm metin ön işleme adımlarını sırasıyla uygular:\n",
    "        - Özel karakterleri kaldırma\n",
    "        - Sayıları kaldırma\n",
    "        - Noktalama işaretlerini kaldırma\n",
    "        - Stopword'leri kaldırma\n",
    "        - Lemmatizasyon\n",
    "        - Kısa kelimeleri kaldırma\n",
    "        '''\n",
    "        text = self.remove_special_characters(text)\n",
    "        text = self.remove_numbers(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.lemmatize_text(text)\n",
    "        text = self.remove_short_words(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTwm7V4fDRgx"
   },
   "source": [
    "# Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rHdsLrwxtgfw"
   },
   "outputs": [],
   "source": [
    "class WordRepresentation:\n",
    "    '''\n",
    "    Kelime temsil modellerini oluşturmak için sınıf:\n",
    "    - Word2Vec\n",
    "    - FastText\n",
    "    - TF-IDF\n",
    "    - BERT Embedding\n",
    "    - T5 Embedding\n",
    "    '''\n",
    "    def word2vec(self, sentences, **params):\n",
    "        \"\"\"Word2Vec modelini oluştur ve eğit.\n",
    "        sentences = Giriş \n",
    "        \"\"\"\n",
    "        model = Word2Vec(\n",
    "            sentences=sentences, \n",
    "            vector_size=200, # Varsayılan embedding boyutu\n",
    "            window=5, # Varsayılan pencere boyutu\n",
    "            min_count=2, # Minimum kelime frekansı\n",
    "            workers=4, # Paralel iş parçacığı sayısı\n",
    "            epochs=10\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fasttext(self, sentences, **params):\n",
    "        \"\"\"FastText modelini oluştur ve eğit.\"\"\"\n",
    "        \n",
    "        model = FastText(\n",
    "            sentences=sentences,\n",
    "            vector_size=params.get('vector_size', 200),  # Varsayılan embedding boyutu\n",
    "            window=params.get('window', 5),  # Varsayılan pencere boyutu\n",
    "            min_count=params.get('min_count', 2),  # Minimum kelime frekansı\n",
    "            workers=params.get('workers', 4),  # Paralel iş parçacığı sayısı\n",
    "            epochs = params.get('epochs', 5)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "    def tfidf(self, sentences, **params):\n",
    "        \"\"\"TF-IDF vektörleştiricisini oluştur ve uygula.\"\"\"\n",
    "        vectorizer = TfidfVectorizer(**params)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        return vectorizer, tfidf_matrix\n",
    "\n",
    "    def T5Transformer(self, sentences, batch_size=32, **params):\n",
    "        model_name = \"google/mt5-small\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(device)  # Modeli GPU'ya taşı\n",
    "\n",
    "        embeddings = []  # Embedding listesi\n",
    "\n",
    "        # Batch işlemi\n",
    "        for i in range(0, len(sentences), batch_size): # batch boyutunu start stop fonksiyonu ile ayarla\n",
    "            batch = sentences[i:i + batch_size]  # Batch'i al\n",
    "            # Tokenizer ile batch'i işleme\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=params.get('max_length', 512)  # Uzunluk sınırı\n",
    "            ).to(device)  # Girişleri GPU'ya taşı\n",
    "\n",
    "            with torch.no_grad(): \n",
    "                outputs = model.encoder(**inputs)  # Sadece encoder kısmını çalıştır\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Cümle için ortalama embedding al\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())  # Batch embedding'leri CPU'ya taşı ve kaydet\n",
    "\n",
    "        # Tüm batch'leri birleştir\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "    def BertEmbedding(self, sentences, batch_size=32, **params):\n",
    "        model_name = \"dbmdz/bert-base-turkish-cased\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Model ve tokenizer yükleniyor\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "        # Tüm cümleler için embedding hesaplama\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_sentences = sentences[i:i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch_sentences,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512 # Uzunluk sınırı\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)  # Cümle için ortalama embedding al\n",
    "                all_embeddings.append(embeddings.cpu().numpy()) # Batch embedding'leri CPU'ya taşı ve kaydet\n",
    "\n",
    "        return np.vstack(all_embeddings)  # Batch sonuçlarını birleştir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9g09ba7DiVZ"
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7Nd_Q3mRvTrx"
   },
   "outputs": [],
   "source": [
    "class DeepLearning:\n",
    "    def LSTM(self, vectors, labels, embedding_dim=200, hidden_size=256, epochs=10, batch_size=16, dropout=0.3, learning_rate=0.001):\n",
    "        '''\n",
    "        LSTM modeli oluştur ve %80 eğitim, %20 test ile eğit (Keras kullanılarak).\n",
    "\n",
    "        Args:\n",
    "            vectors: Giriş embedding vektörleri (ör. Word2Vec, FastText, TF-IDF, Transformer).\n",
    "            labels: Sınıf etiketleri.\n",
    "            embedding_dim: Embedding boyutu.\n",
    "            hidden_size: LSTM gizli birim boyutu.\n",
    "            epochs: Eğitim epoch sayısı.\n",
    "            batch_size: Batch boyutu.\n",
    "            learning_rate: Öğrenme oranı.\n",
    "\n",
    "        Returns:\n",
    "            Test metriklerini içeren bir sözlük.\n",
    "        '''\n",
    "\n",
    "        vectors = vectors.reshape((vectors.shape[0], 1 , vectors.shape[1]))\n",
    "        output_size = len(np.unique(labels))\n",
    "        binary_classification = (output_size == 2)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            vectors, labels,\n",
    "            test_size = 0.2,\n",
    "            random_state = 42,\n",
    "            stratify=labels\n",
    "        )\n",
    "\n",
    "        # Dengesiz veride sinif agirliklarinin duzenlenmesi\n",
    "        class_weights = compute_class_weight('balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "        print(f\"Sınıf Ağırlıkları: {class_weights}\")\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(hidden_size, return_sequences = False, input_shape=(1, embedding_dim)))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(3, activation='softmax' if not binary_classification else 'sigmoid'))\n",
    "\n",
    "        loss = 'sparse_categorical_crossentropy' if not binary_classification else 'binary_crossentropy'\n",
    "        model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Modeli eğit\n",
    "        model.fit(\n",
    "            np.array(X_train),\n",
    "            np.array(y_train),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Test seti tahmini\n",
    "        y_pred = model.predict(np.array(X_test), verbose=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1) if not binary_classification else (y_pred > 0.5).astype(int)\n",
    "\n",
    "        # Performans metriklerini hesapla\n",
    "        average_type = 'weighted'\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average=average_type),\n",
    "            'recall': recall_score(y_test, y_pred, average=average_type),\n",
    "            'f1_score': f1_score(y_test, y_pred, average=average_type)\n",
    "        }\n",
    "\n",
    "        print(\"Test Sonuçları:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def CNN(self, vectors, labels, embedding_dim=200, num_filters=64, kernel_size=5, pool_size=2,\n",
    "            dense_units=64, dropout_rate=0.2, epochs=7, batch_size=16, learning_rate=0.001):\n",
    "        '''\n",
    "        CNN modeli oluştur ve %80 eğitim, %20 test ile eğit (Keras kullanılarak).\n",
    "\n",
    "        Args:\n",
    "            vectors: Giriş embedding vektörleri (ör. Word2Vec, FastText, GloVe, TF-IDF, Transformer).\n",
    "            labels: Sınıf etiketleri.\n",
    "            embedding_dim: Embedding boyutu.\n",
    "            num_filters: Convolutional layer filtre sayısı.\n",
    "            kernel_size: Kernel boyutu.\n",
    "            pool_size: Max pooling boyutu.\n",
    "            dense_units: Fully connected layer'daki nöron sayısı.\n",
    "            dropout_rate: Dropout oranı.\n",
    "            epochs: Eğitim epoch sayısı.\n",
    "            batch_size: Batch boyutu.\n",
    "            learning_rate: Öğrenme oranı.\n",
    "\n",
    "        Returns:\n",
    "            Test metriklerini içeren bir sözlük.\n",
    "        '''\n",
    "\n",
    "        # Veriyi uygun şekle getirme\n",
    "        vectors = vectors.reshape((vectors.shape[0], vectors.shape[1], 1))\n",
    "        output_size = len(np.unique(labels))\n",
    "        binary_classification = (output_size == 2)\n",
    "\n",
    "        # Eğitim ve test seti ayırma\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            vectors, labels,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=labels\n",
    "        )\n",
    "\n",
    "        # Dengesiz veride sınıf ağırlıklarının düzenlenmesi\n",
    "        class_weights = compute_class_weight('balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "        print(f\"Sınıf Ağırlıkları: {class_weights}\")\n",
    "\n",
    "        # Model oluşturma\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(num_filters, kernel_size, activation='relu', input_shape=(embedding_dim, 1)))\n",
    "        model.add(MaxPooling1D(pool_size))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(output_size, activation='softmax' if not binary_classification else 'sigmoid'))\n",
    "\n",
    "        loss = 'sparse_categorical_crossentropy' if not binary_classification else 'binary_crossentropy'\n",
    "        model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Modeli eğit\n",
    "        model.fit(\n",
    "            np.array(X_train),\n",
    "            np.array(y_train),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Test seti tahmini\n",
    "        y_pred = model.predict(np.array(X_test), verbose=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1) if not binary_classification else (y_pred > 0.5).astype(int)\n",
    "\n",
    "        # Performans metriklerini hesapla\n",
    "        average_type = 'weighted'\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average=average_type),\n",
    "            'recall': recall_score(y_test, y_pred, average=average_type),\n",
    "            'f1_score': f1_score(y_test, y_pred, average=average_type)\n",
    "        }\n",
    "\n",
    "        print(\"Test Sonuçları:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def CapsuleNetwork(self, embeddings, labels, batch_size=16, epochs=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Capsule Network tabanlı bir sınıflandırıcı: Embedding vektörlerini alır ve kendi içinde sınıflama yapar.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Giriş embedding vektörleri (numpy array formatında).\n",
    "            labels: Sınıf etiketleri (numpy array ya da liste).\n",
    "            batch_size: Batch boyutu.\n",
    "            epochs: Eğitim epoch sayısı.\n",
    "            learning_rate: Öğrenme oranı.\n",
    "\n",
    "        Returns:\n",
    "            Test metriklerini içeren bir sözlük.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Eğitim ve test setlerini ayırma\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "\n",
    "        # Sınıf ağırlıklarını hesaplama\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=torch.unique(torch.tensor(labels)).cpu().numpy(),\n",
    "            y=labels\n",
    "        )\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "        # TensorDataset ve DataLoader\n",
    "        train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "        test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "        # Capsule Network Model\n",
    "        class CapsuleLayer(nn.Module):\n",
    "            def __init__(self, input_dim, output_dim, num_capsules, routing_iterations):\n",
    "                \"\"\"\n",
    "                input_dim: Giriş veri boyutu (özellik sayısı)\n",
    "                output_dim: Her kapsülün çıkış boyutu\n",
    "                num_capsules: Kapsül sayısı (kaç tane kapsül çıkışı olacağı)\n",
    "                routing_iterations: Dinamik yönlendirme (routing) iterasyon sayısı\n",
    "                \"\"\"\n",
    "                super(CapsuleLayer, self).__init__()\n",
    "                self.num_capsules = num_capsules  # Kapsül sayısını sakla\n",
    "                self.routing_iterations = routing_iterations  # Yönlendirme tekrar sayısını sakla\n",
    "        \n",
    "                # Ağırlık matrisi (W) oluşturuluyor\n",
    "                self.W = nn.Parameter(torch.randn(1, input_dim, num_capsules * output_dim))\n",
    "\n",
    "            def forward(self, x):\n",
    "                \"\"\"\n",
    "                x: Giriş verisi (batch_size, input_dim)\n",
    "                \"\"\"\n",
    "                batch_size = x.size(0)  # Batch boyutunu al\n",
    "        \n",
    "                # Kapsüllere bağlanacak ağırlık matrisini uygula\n",
    "                x = torch.matmul(x.unsqueeze(1), self.W).squeeze(1)\n",
    "        \n",
    "                # Tensor boyutunu ayarla (batch_size, num_capsules, output_dim)\n",
    "                x = x.view(batch_size, self.num_capsules, -1)\n",
    "        \n",
    "                # L2 norm ile vektörlerin uzunluğunu normalize et\n",
    "                x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "        \n",
    "                return x\n",
    "\n",
    "        class CapsuleNetworkModel(nn.Module):\n",
    "            def __init__(self, input_dim, num_classes):\n",
    "                \"\"\"\n",
    "                input_dim: Modelin giriş boyutu\n",
    "                num_classes: Çıkış katmanındaki sınıf sayısı (kaç sınıfa sınıflandırma yapılacağı)\n",
    "                \"\"\"\n",
    "                super(CapsuleNetworkModel, self).__init__()\n",
    "        \n",
    "                # Kapsül Katmanı oluşturuluyor (10 kapsül, her biri 16 boyutlu çıkış üretiyor)\n",
    "                self.capsule = CapsuleLayer(input_dim, 16, num_capsules=10, routing_iterations=3)\n",
    "        \n",
    "                # Tam bağlı (Dense) katman ekleniyor\n",
    "                self.fc = nn.Linear(16 * 10, num_classes)  # 10 kapsül * 16 boyut -> num_classes\n",
    "\n",
    "            def forward(self, x):\n",
    "                \"\"\"\n",
    "                x: Giriş verisi (batch_size, input_dim)\n",
    "                \"\"\"\n",
    "                x = self.capsule(x)  # Kapsül katmanından geçir\n",
    "        \n",
    "                # Düzleştirme işlemi (Flatten)\n",
    "                x = x.view(x.size(0), -1)\n",
    "        \n",
    "                # Tam bağlı katmandan geçirerek sınıflandırma yap\n",
    "                x = self.fc(x)\n",
    "        \n",
    "                return x\n",
    "\n",
    "        num_labels = len(set(labels))\n",
    "        binary_classification = (num_labels == 2)\n",
    "\n",
    "        model = CapsuleNetworkModel(embeddings.shape[1], 1 if binary_classification else num_labels).to(device)\n",
    "\n",
    "        # Optimizasyon ve loss fonksiyonu\n",
    "        if binary_classification:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Eğitim döngüsü\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch_embeddings, batch_labels = [b.to(device) for b in batch]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_embeddings).squeeze()\n",
    "                loss = loss_fn(outputs, batch_labels.float() if binary_classification else batch_labels)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Değerlendirme\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch_embeddings, batch_labels = [b.to(device) for b in batch]\n",
    "                outputs = model(batch_embeddings).squeeze()\n",
    "                if binary_classification:\n",
    "                    predictions = (torch.sigmoid(outputs) > 0.5).cpu().numpy()\n",
    "                else:\n",
    "                    predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                y_pred.extend(predictions)\n",
    "                y_true.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        # Performans metriklerini hesapla\n",
    "        average_type = \"binary\" if binary_classification else \"weighted\"\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, average=average_type),\n",
    "            \"recall\": recall_score(y_true, y_pred, average=average_type),\n",
    "            \"f1_score\": f1_score(y_true, y_pred, average=average_type)\n",
    "        }\n",
    "\n",
    "        print(\"Test Sonuçları:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxB7GVccDn2e"
   },
   "source": [
    "# Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O07OZId1vUqq"
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier:\n",
    "    def BERT(self, embeddings, labels, model_name=\"dbmdz/bert-base-turkish-cased\", batch_size=16, epochs=5, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        BERT tabanlı sınıflandırma modeli oluşturur ve eğitir.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Gömme vektörleri (numpy array formatında).\n",
    "            labels: Sınıf etiketleri (numpy array ya da liste).\n",
    "            model_name: Kullanılacak BERT modelinin adı.\n",
    "            batch_size: Batch boyutu.\n",
    "            epochs: Eğitim epoch sayısı.\n",
    "            learning_rate: Öğrenme oranı.\n",
    "\n",
    "        Returns:\n",
    "            Test metriklerini içeren bir sözlük.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(f\"{model_name} modeli ile sınıflandırma modeli eğitiliyor...\")\n",
    "\n",
    "        # Eğitim ve test setlerini ayırma\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "\n",
    "        # Sınıf ağırlıklarını hesaplama\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=torch.unique(torch.tensor(labels)).cpu().numpy(),\n",
    "            y=labels\n",
    "        )\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "        # TensorDataset ve DataLoader\n",
    "        train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "        test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        num_labels = len(set(labels))\n",
    "        binary_classification = (num_labels == 2)\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(embeddings.shape[1], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1 if binary_classification else num_labels),\n",
    "            nn.Sigmoid() if binary_classification else nn.Softmax(dim=1)\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizasyon ve loss fonksiyonu\n",
    "        if binary_classification:\n",
    "            loss_fn = nn.BCELoss()\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Eğitim döngüsü\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch_embeddings, batch_labels = [b.to(device) for b in batch]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_embeddings).squeeze()\n",
    "                loss = loss_fn(outputs, batch_labels.float() if binary_classification else batch_labels)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Değerlendirme\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch_embeddings, batch_labels = [b.to(device) for b in batch]\n",
    "                outputs = model(batch_embeddings).squeeze()\n",
    "                if binary_classification:\n",
    "                    predictions = (outputs > 0.5).cpu().numpy()\n",
    "                else:\n",
    "                    predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                y_pred.extend(predictions)\n",
    "                y_true.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        # Performans metriklerini hesapla\n",
    "        average_type = 'binary' if binary_classification else 'weighted'\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average=average_type),\n",
    "            'recall': recall_score(y_true, y_pred, average=average_type),\n",
    "            'f1_score': f1_score(y_true, y_pred, average=average_type)\n",
    "        }\n",
    "\n",
    "        print(\"Test Sonuçları:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def T5(self, embeddings, labels, model_name=\"google/mt5-small\", batch_size=16, epochs=5, learning_rate=0.001):\n",
    "            \"\"\"\n",
    "            T5 tabanlı sınıflandırma modeli oluşturur ve eğitir.\n",
    "\n",
    "            Args:\n",
    "                embeddings: Gömme vektörleri (numpy array formatında).\n",
    "                labels: Sınıf etiketleri (numpy array ya da liste).\n",
    "                model_name: Kullanılacak T5 modelinin adı.\n",
    "                batch_size: Batch boyutu.\n",
    "                epochs: Eğitim epoch sayısı.\n",
    "                learning_rate: Öğrenme oranı.\n",
    "\n",
    "            Returns:\n",
    "                Test metriklerini içeren bir sözlük.\n",
    "            \"\"\"\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            print(f\"{model_name} modeli ile sınıflandırma modeli eğitiliyor...\")\n",
    "\n",
    "            # Eğitim ve test setlerini ayırma\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "            )\n",
    "\n",
    "            # Sınıf ağırlıklarını hesaplama\n",
    "            class_weights = compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=torch.unique(torch.tensor(labels)).cpu().numpy(),\n",
    "                y=labels\n",
    "            )\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "            # TensorDataset ve DataLoader\n",
    "            train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "            test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "            train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "            num_labels = len(set(labels))\n",
    "            binary_classification = (num_labels == 2)\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(embeddings.shape[1], 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(128, 1 if binary_classification else num_labels),\n",
    "                nn.Sigmoid() if binary_classification else nn.Softmax(dim=1)\n",
    "            ).to(device)\n",
    "\n",
    "            # Optimizasyon ve loss fonksiyonu\n",
    "            if binary_classification:\n",
    "                loss_fn = nn.BCELoss()\n",
    "            else:\n",
    "                loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Eğitim döngüsü\n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                for batch in train_loader:\n",
    "                    batch_embeddings, batch_labels = [b.to(device) for b in batch]\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_embeddings).squeeze()\n",
    "                    loss = loss_fn(outputs, batch_labels.float() if binary_classification else batch_labels)\n",
    "                    total_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "            # Değerlendirme\n",
    "            model.eval()\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    batch_embeddings, batch_labels = [b.to(device) for b in batch]\n",
    "                    outputs = model(batch_embeddings).squeeze()\n",
    "                    if binary_classification:\n",
    "                        predictions = (outputs > 0.5).cpu().numpy()\n",
    "                    else:\n",
    "                        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    y_pred.extend(predictions)\n",
    "                    y_true.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "            # Performans metriklerini hesapla\n",
    "            average_type = 'binary' if binary_classification else 'weighted'\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "                'precision': precision_score(y_true, y_pred, average=average_type),\n",
    "                'recall': recall_score(y_true, y_pred, average=average_type),\n",
    "                'f1_score': f1_score(y_true, y_pred, average=average_type)\n",
    "            }\n",
    "\n",
    "            print(\"Test Sonuçları:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "            return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMf8X_yUX-jP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpgWZbErDsTP"
   },
   "source": [
    "**Veri Yükleme**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3TTDBg8tvxkO"
   },
   "outputs": [],
   "source": [
    "# Veri yükleme\n",
    "data = pd.read_excel(\"/content/orj_aigen_paraphrased.xlsx\") # Verinin yüklenmesi\n",
    "data = data.dropna(subset=[\"label\", \"oz\"]) # boş satır kontrolü\n",
    "preprocessor = TextPreprocessor() # Metin ön işleme sınıfının değişkene atanması\n",
    "data[\"oz\"] = data[\"oz\"].apply(preprocessor.preprocess) # metin ön işleme adımının çalıştırılması \n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in data[\"oz\"].astype(str)] # Metinlerin tokenize edilmesi\n",
    "labels = data[\"label\"].to_numpy()\n",
    "sentences = data[\"oz\"].astype(str).tolist() # Transformer modelleri için cümleler tokenize edilmeden liste halinde verilmiştir. Modellerde auto tokenizer bulunmaktadır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vvw0ZsRrD61-"
   },
   "source": [
    "**Sınıfların Yüklenmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HpJqBjrbwvEn"
   },
   "outputs": [],
   "source": [
    "wr = WordRepresentation() # Kelime temsil yöntemleri sınıfının değişkene atanması \n",
    "dl = DeepLearning() # Derin öğrenme algoritmaları sınıfının değişkene atanması\n",
    "tcr = TransformerClassifier() # Transformer yapılı sınıflayıcıların sınıfının değişkene atanması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReT-RGHvE3t_"
   },
   "source": [
    "**Word2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kg3FXKEhvd43"
   },
   "outputs": [],
   "source": [
    "w2v_model = wr.word2vec(tokenized_sentences)\n",
    "\n",
    "# Embedding vektörlerini oluşturma\n",
    "embedding_dim = 200\n",
    "w2v_vectors = np.array([\n",
    "  np.mean([w2v_model.wv[word] for word in sentence if word in w2v_model.wv] or [np.zeros(embedding_dim)], axis=0)\n",
    "  for sentence in tokenized_sentences\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBXaEimCFL7R"
   },
   "source": [
    "**Fast-Text Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4XPhXWbGFLXs"
   },
   "outputs": [],
   "source": [
    "fasttext_model = wr.fasttext(tokenized_sentences)\n",
    "\n",
    "embedding_dim = 200\n",
    "ft_vectors = np.array([\n",
    "    np.mean([fasttext_model.wv[word] for word in sentence if word in fasttext_model.wv] or [np.zeros(embedding_dim)], axis=0)\n",
    "    for sentence in tokenized_sentences\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn9oG_FsFAf4"
   },
   "source": [
    "**TF-IDF Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OjM5_UkNE_wc"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf_matrix = wr.tfidf(\n",
    "    data[\"oz\"],\n",
    "    max_features=30000,  # Özellik sınırı 30000\n",
    "    max_df=0.9,         # Çok sık geçen kelimeleri filtrele (ör. 90%'dan fazla geçenler)\n",
    "    min_df=2,           # Çok nadir geçen kelimeleri filtrele (ör. 2'den az geçenler)\n",
    ")\n",
    "\n",
    "# TF-IDF matrisini dense (yoğun) formata çevir\n",
    "tfidf_vectors = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnBBBpdQFX7c"
   },
   "source": [
    "**Bert Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UTiTfK5FbL-",
    "outputId": "feebc888-969c-4c43-b209-7bc8b3205c5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cihaz ayarı\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU varsa kullanılır, yoksa CPU\n",
    "\n",
    "# BERT embedding işlemi\n",
    "wr = WordRepresentation()\n",
    "bert_embeddings = wr.BertEmbedding(\n",
    "    sentences=sentences,\n",
    "    device=device  # Cihaz bilgisi ekleniyor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UluFQbgmGQh0"
   },
   "source": [
    "**T5 Embedding**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYA2ihb9GQB1",
    "outputId": "3744ef41-32de-4ed9-e52d-d3d89b93fcab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t5_embeddings = wr.T5Transformer(sentences=sentences, device=device, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etzbb5jUGtC2"
   },
   "source": [
    "**Word2Vec - LSTM Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ne3be_K4uRB",
    "outputId": "b821d7e7-736c-4974-ce8c-6b79d725775a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.6025 - loss: 0.7559\n",
      "Epoch 2/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7202 - loss: 0.5794\n",
      "Epoch 3/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7511 - loss: 0.5381\n",
      "Epoch 4/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.5104\n",
      "Epoch 5/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7763 - loss: 0.4889\n",
      "Epoch 6/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7870 - loss: 0.4664\n",
      "Epoch 7/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7950 - loss: 0.4479\n",
      "Epoch 8/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8009 - loss: 0.4381\n",
      "Epoch 9/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8146 - loss: 0.4176\n",
      "Epoch 10/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8134 - loss: 0.4005\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7979\n",
      "Precision: 0.8366\n",
      "Recall: 0.7979\n",
      "F1_score: 0.8103\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.LSTM(w2v_vectors, labels, dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i1rVfE3JWx5"
   },
   "source": [
    "**FastText - LSTM Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzv9lqGtJV-3",
    "outputId": "dd05c6ca-1f32-4868-ad38-d067f5bd1baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.6049 - loss: 0.7795\n",
      "Epoch 2/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7122 - loss: 0.6103\n",
      "Epoch 3/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7366 - loss: 0.5646\n",
      "Epoch 4/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7518 - loss: 0.5347\n",
      "Epoch 5/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.5026\n",
      "Epoch 6/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7807 - loss: 0.4792\n",
      "Epoch 7/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7849 - loss: 0.4652\n",
      "Epoch 8/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7982 - loss: 0.4445\n",
      "Epoch 9/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8016 - loss: 0.4254\n",
      "Epoch 10/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8069 - loss: 0.4137\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7779\n",
      "Precision: 0.8352\n",
      "Recall: 0.7779\n",
      "F1_score: 0.7919\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.LSTM(ft_vectors, labels, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJFiyD-fJl6-"
   },
   "source": [
    "**TF-IDF - LSTM Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6BN3LnE8Jlbv",
    "outputId": "34ea5dcb-1b57-4ad6-fc1b-a330770c12d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 26ms/step - accuracy: 0.7303 - loss: 0.6374\n",
      "Epoch 2/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.8918 - loss: 0.2872\n",
      "Epoch 3/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.9266 - loss: 0.1953\n",
      "Epoch 4/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.9394 - loss: 0.1506\n",
      "Epoch 5/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.9499 - loss: 0.1154\n",
      "Epoch 6/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.9600 - loss: 0.0932\n",
      "Epoch 7/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.9641 - loss: 0.0835\n",
      "Epoch 8/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 26ms/step - accuracy: 0.9689 - loss: 0.0734\n",
      "Epoch 9/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 26ms/step - accuracy: 0.9709 - loss: 0.0662\n",
      "Epoch 10/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26ms/step - accuracy: 0.9755 - loss: 0.0591\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8711\n",
      "Precision: 0.8777\n",
      "Recall: 0.8711\n",
      "F1_score: 0.8737\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.LSTM(\n",
    "    tfidf_vectors, labels, embedding_dim=tfidf_vectors.shape[1], hidden_size=256, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME8xt8ucIn4R"
   },
   "source": [
    "**Bert Embedding - LSTM Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqyTSOuyImL1",
    "outputId": "07945742-838b-4bd3-d4cb-e29d5ac43e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.6466 - loss: 0.6958\n",
      "Epoch 2/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7374 - loss: 0.5340\n",
      "Epoch 3/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.5078\n",
      "Epoch 4/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7530 - loss: 0.5214\n",
      "Epoch 5/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.6613 - loss: 0.6788\n",
      "Epoch 6/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7689 - loss: 0.4895\n",
      "Epoch 7/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7846 - loss: 0.4667\n",
      "Epoch 8/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7941 - loss: 0.4536\n",
      "Epoch 9/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7844 - loss: 0.4606\n",
      "Epoch 10/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7484 - loss: 0.5092\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7985\n",
      "Precision: 0.8390\n",
      "Recall: 0.7985\n",
      "F1_score: 0.8099\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.LSTM(\n",
    "    vectors=torch.tensor(bert_embeddings, device=device).cpu().numpy(),  # Embeddingleri CPU'ya alıyoruz\n",
    "    labels=labels, hidden_size=256, epochs=10, batch_size=16, dropout=0.3, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhjuWMSqHw8N"
   },
   "source": [
    "**T5 Embedding - LSTM Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8_L7_2nHwMc",
    "outputId": "29377042-2ba7-4617-f9d5-ab07ed08652f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.6379 - loss: 0.6713\n",
      "Epoch 2/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7071 - loss: 0.5201\n",
      "Epoch 3/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7332 - loss: 0.4963\n",
      "Epoch 4/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7421 - loss: 0.4778\n",
      "Epoch 5/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7535 - loss: 0.4647\n",
      "Epoch 6/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.4546\n",
      "Epoch 7/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7759 - loss: 0.4437\n",
      "Epoch 8/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7826 - loss: 0.4360\n",
      "Epoch 9/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7821 - loss: 0.4360\n",
      "Epoch 10/10\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7857 - loss: 0.4305\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8117\n",
      "Precision: 0.8501\n",
      "Recall: 0.8117\n",
      "F1_score: 0.8184\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.LSTM(\n",
    "    vectors=t5_embeddings,\n",
    "    labels=labels,\n",
    "    embedding_dim=t5_embeddings.shape[1],\n",
    "    hidden_size=256,\n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKQc9eCaKrMK"
   },
   "source": [
    "**Word2Vec - CNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMtYeFFwKqg3",
    "outputId": "6ed618b8-a7db-4806-df6f-de4eb6953c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5849 - loss: 0.8166\n",
      "Epoch 2/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6548 - loss: 0.6551\n",
      "Epoch 3/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6803 - loss: 0.6226\n",
      "Epoch 4/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7057 - loss: 0.5950\n",
      "Epoch 5/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7156 - loss: 0.5770\n",
      "Epoch 6/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7232 - loss: 0.5725\n",
      "Epoch 7/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7247 - loss: 0.5568\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7464\n",
      "Precision: 0.8029\n",
      "Recall: 0.7464\n",
      "F1_score: 0.7568\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CNN(w2v_vectors, labels, dropout_rate=0.3, epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ3zqhaqLFsX"
   },
   "source": [
    "**FastText - CNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3Fy9O9wLMAz",
    "outputId": "71c4d17f-3445-4681-c85e-79f560013d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.5801 - loss: 0.8257\n",
      "Epoch 2/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6406 - loss: 0.6799\n",
      "Epoch 3/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6616 - loss: 0.6483\n",
      "Epoch 4/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6787 - loss: 0.6215\n",
      "Epoch 5/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6917 - loss: 0.6080\n",
      "Epoch 6/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.6986 - loss: 0.5991\n",
      "Epoch 7/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7017 - loss: 0.5868\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7155\n",
      "Precision: 0.7796\n",
      "Recall: 0.7155\n",
      "F1_score: 0.7244\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CNN(ft_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjuQEsxfLnDK"
   },
   "source": [
    "**TFIDF - CNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YoMlN_-DLrul",
    "outputId": "f82e8374-8e3c-49f5-af8d-3f17c1c2cad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 28ms/step - accuracy: 0.6972 - loss: 0.6914\n",
      "Epoch 2/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.8761 - loss: 0.3161\n",
      "Epoch 3/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.9172 - loss: 0.2208\n",
      "Epoch 4/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.9434 - loss: 0.1613\n",
      "Epoch 5/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 28ms/step - accuracy: 0.9521 - loss: 0.1391\n",
      "Epoch 6/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 28ms/step - accuracy: 0.9598 - loss: 0.1192\n",
      "Epoch 7/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 28ms/step - accuracy: 0.9640 - loss: 0.1091\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8865\n",
      "Precision: 0.8869\n",
      "Recall: 0.8865\n",
      "F1_score: 0.8866\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CNN(tfidf_vectors, labels, embedding_dim=tfidf_vectors.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSzJ0YAWMcid"
   },
   "source": [
    "**BERT Embedding - CNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFRDAHwoMcQv",
    "outputId": "dbc09cb8-8331-4b1a-f5f5-b6a7e2c5b286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.6295 - loss: 0.7385\n",
      "Epoch 2/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7131 - loss: 0.5673\n",
      "Epoch 3/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7338 - loss: 0.5284\n",
      "Epoch 4/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7500 - loss: 0.5052\n",
      "Epoch 5/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7685 - loss: 0.4871\n",
      "Epoch 6/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7788 - loss: 0.4687\n",
      "Epoch 7/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7820 - loss: 0.4620\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7306\n",
      "Precision: 0.8293\n",
      "Recall: 0.7306\n",
      "F1_score: 0.7384\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CNN(\n",
    "    vectors=torch.tensor(bert_embeddings, device=device).cpu().numpy(),  # Embeddingleri CPU'ya alıyoruz\n",
    "    labels=labels, epochs=7, batch_size=16, embedding_dim=bert_embeddings.shape[1], learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beOeBm_DMQkr"
   },
   "source": [
    "**T5 Embedding - CNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oyQoEOxMOdP",
    "outputId": "aad86c76-ec38-4932-ae7f-20ea75541ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıf Ağırlıkları: {0: 3.666833583708897, 1: 0.7333299951596481, 2: 0.7333299951596481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.6361 - loss: 0.6742\n",
      "Epoch 2/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7088 - loss: 0.5197\n",
      "Epoch 3/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7338 - loss: 0.4949\n",
      "Epoch 4/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.4690\n",
      "Epoch 5/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7600 - loss: 0.4627\n",
      "Epoch 6/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4533\n",
      "Epoch 7/7\n",
      "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7750 - loss: 0.4509\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8142\n",
      "Precision: 0.8403\n",
      "Recall: 0.8142\n",
      "F1_score: 0.8236\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.LSTM(\n",
    "    vectors=t5_embeddings,\n",
    "    labels=labels,\n",
    "    embedding_dim=t5_embeddings.shape[1],\n",
    "    epochs=7,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLmMcRf_NAQW"
   },
   "source": [
    "**Word2Vec - CapsuleNetwork Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfFZoEFwNF9y",
    "outputId": "a168ae55-3910-4b84-ca3c-d086a9f4adb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7482\n",
      "Epoch 2/10, Loss: 0.6417\n",
      "Epoch 3/10, Loss: 0.6083\n",
      "Epoch 4/10, Loss: 0.5858\n",
      "Epoch 5/10, Loss: 0.5689\n",
      "Epoch 6/10, Loss: 0.5534\n",
      "Epoch 7/10, Loss: 0.5436\n",
      "Epoch 8/10, Loss: 0.5327\n",
      "Epoch 9/10, Loss: 0.5223\n",
      "Epoch 10/10, Loss: 0.5151\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7765\n",
      "Precision: 0.8057\n",
      "Recall: 0.7765\n",
      "F1_score: 0.7853\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CapsuleNetwork(w2v_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ-V-WOYNP7C"
   },
   "source": [
    "**FastText - CapsuleNetwork Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4q0AnrlNKMJ",
    "outputId": "991fbfae-fa9b-439a-c840-50ef7271ff6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7814\n",
      "Epoch 2/10, Loss: 0.6744\n",
      "Epoch 3/10, Loss: 0.6385\n",
      "Epoch 4/10, Loss: 0.6130\n",
      "Epoch 5/10, Loss: 0.5971\n",
      "Epoch 6/10, Loss: 0.5814\n",
      "Epoch 7/10, Loss: 0.5706\n",
      "Epoch 8/10, Loss: 0.5581\n",
      "Epoch 9/10, Loss: 0.5494\n",
      "Epoch 10/10, Loss: 0.5394\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7760\n",
      "Precision: 0.8058\n",
      "Recall: 0.7760\n",
      "F1_score: 0.7845\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CapsuleNetwork(ft_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVThjg5CNiqe"
   },
   "source": [
    "**TFIDF - CapsuleNetwork Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZupuyJaGNtDL",
    "outputId": "fd53ae19-0be6-4d8f-ae10-8b7a3bc586c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6781\n",
      "Epoch 2/10, Loss: 0.3796\n",
      "Epoch 3/10, Loss: 0.2751\n",
      "Epoch 4/10, Loss: 0.2135\n",
      "Epoch 5/10, Loss: 0.1718\n",
      "Epoch 6/10, Loss: 0.1401\n",
      "Epoch 7/10, Loss: 0.1141\n",
      "Epoch 8/10, Loss: 0.0955\n",
      "Epoch 9/10, Loss: 0.0788\n",
      "Epoch 10/10, Loss: 0.0665\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8607\n",
      "Precision: 0.8614\n",
      "Recall: 0.8607\n",
      "F1_score: 0.8610\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CapsuleNetwork(tfidf_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxyZr41-Nyd8"
   },
   "source": [
    "**BERT Embedding - CapsuleNetwork Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_6mofzaN5Oo",
    "outputId": "7cfade30-e8e7-4c76-df23-66f767e7597a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6118\n",
      "Epoch 2/10, Loss: 0.4919\n",
      "Epoch 3/10, Loss: 0.4568\n",
      "Epoch 4/10, Loss: 0.4348\n",
      "Epoch 5/10, Loss: 0.4181\n",
      "Epoch 6/10, Loss: 0.4051\n",
      "Epoch 7/10, Loss: 0.3946\n",
      "Epoch 8/10, Loss: 0.3853\n",
      "Epoch 9/10, Loss: 0.3751\n",
      "Epoch 10/10, Loss: 0.3618\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8582\n",
      "Precision: 0.8655\n",
      "Recall: 0.8582\n",
      "F1_score: 0.8608\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CapsuleNetwork(\n",
    "    embeddings=torch.tensor(bert_embeddings, device=device).cpu().numpy(),  # Embeddingleri CPU'ya alıyoruz\n",
    "    labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVQv19Y_J2fA"
   },
   "source": [
    "**T5 Embedding - CapsuleNetwork Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNTWI8kzqPRM",
    "outputId": "2ed42028-5905-464d-b3d9-6376fef3216f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6173\n",
      "Epoch 2/10, Loss: 0.5048\n",
      "Epoch 3/10, Loss: 0.4742\n",
      "Epoch 4/10, Loss: 0.4574\n",
      "Epoch 5/10, Loss: 0.4418\n",
      "Epoch 6/10, Loss: 0.4297\n",
      "Epoch 7/10, Loss: 0.4203\n",
      "Epoch 8/10, Loss: 0.4138\n",
      "Epoch 9/10, Loss: 0.4082\n",
      "Epoch 10/10, Loss: 0.4053\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8531\n",
      "Precision: 0.8506\n",
      "Recall: 0.8531\n",
      "F1_score: 0.8514\n"
     ]
    }
   ],
   "source": [
    "metrics = dl.CapsuleNetwork(\n",
    "    embeddings=t5_embeddings,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FsnYHCQTRP8"
   },
   "source": [
    "**Word2Vec - BERT Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wy0CJqTBUIr_",
    "outputId": "90992ee5-1048-4c24-b2ad-dd59ab8a62a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbmdz/bert-base-turkish-cased modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/10, Loss: 0.8944\n",
      "Epoch 2/10, Loss: 0.8367\n",
      "Epoch 3/10, Loss: 0.8208\n",
      "Epoch 4/10, Loss: 0.8100\n",
      "Epoch 5/10, Loss: 0.8010\n",
      "Epoch 6/10, Loss: 0.7957\n",
      "Epoch 7/10, Loss: 0.7901\n",
      "Epoch 8/10, Loss: 0.7866\n",
      "Epoch 9/10, Loss: 0.7828\n",
      "Epoch 10/10, Loss: 0.7792\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7724\n",
      "Precision: 0.8101\n",
      "Recall: 0.7724\n",
      "F1_score: 0.7822\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.BERT(w2v_vectors, labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB2czDpFUtM0"
   },
   "source": [
    "**FastText - BERT Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-wQPZvSUbKx",
    "outputId": "108b3762-6688-436b-9c33-12d7e5d87fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbmdz/bert-base-turkish-cased modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/10, Loss: 0.9063\n",
      "Epoch 2/10, Loss: 0.8601\n",
      "Epoch 3/10, Loss: 0.8432\n",
      "Epoch 4/10, Loss: 0.8322\n",
      "Epoch 5/10, Loss: 0.8245\n",
      "Epoch 6/10, Loss: 0.8175\n",
      "Epoch 7/10, Loss: 0.8117\n",
      "Epoch 8/10, Loss: 0.8066\n",
      "Epoch 9/10, Loss: 0.8030\n",
      "Epoch 10/10, Loss: 0.7990\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7597\n",
      "Precision: 0.8028\n",
      "Recall: 0.7597\n",
      "F1_score: 0.7698\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.BERT(ft_vectors, labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2JK7osHU2r8"
   },
   "source": [
    "**TF IDF - BERT Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pV20l--nU2SF",
    "outputId": "4091f341-30c6-4e99-f29b-23169eeeb2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbmdz/bert-base-turkish-cased modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/7, Loss: 0.7999\n",
      "Epoch 2/7, Loss: 0.6759\n",
      "Epoch 3/7, Loss: 0.6443\n",
      "Epoch 4/7, Loss: 0.6257\n",
      "Epoch 5/7, Loss: 0.6153\n",
      "Epoch 6/7, Loss: 0.6060\n",
      "Epoch 7/7, Loss: 0.6012\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8675\n",
      "Precision: 0.8740\n",
      "Recall: 0.8675\n",
      "F1_score: 0.8701\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.BERT(tfidf_vectors, labels, epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4KGmRk4VNl2"
   },
   "source": [
    "**BERT - BERT Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOJY39CpVR0P",
    "outputId": "3190cf2b-e894-44c8-d8de-55e17a224d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbmdz/bert-base-turkish-cased modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/5, Loss: 0.8389\n",
      "Epoch 2/5, Loss: 0.7919\n",
      "Epoch 3/5, Loss: 0.7785\n",
      "Epoch 4/5, Loss: 0.7702\n",
      "Epoch 5/5, Loss: 0.7667\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7982\n",
      "Precision: 0.8390\n",
      "Recall: 0.7982\n",
      "F1_score: 0.8076\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.BERT(bert_embeddings, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCcrYg-eVTf1"
   },
   "source": [
    "**T5 - BERT Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6ZD5Ng7VXVF",
    "outputId": "d4ab14de-858e-4ee8-f564-b89cbeadf11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbmdz/bert-base-turkish-cased modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/5, Loss: 0.8547\n",
      "Epoch 2/5, Loss: 0.8080\n",
      "Epoch 3/5, Loss: 0.7931\n",
      "Epoch 4/5, Loss: 0.7863\n",
      "Epoch 5/5, Loss: 0.7784\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8001\n",
      "Precision: 0.8207\n",
      "Recall: 0.8001\n",
      "F1_score: 0.8080\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.BERT(t5_embeddings, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4yOt7xVWNbl"
   },
   "source": [
    "**Word2Vec - T5 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-zF6wS7V2ls",
    "outputId": "d1def5b5-8ff3-492d-b771-422dcaca4d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/mt5-small modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/10, Loss: 0.8962\n",
      "Epoch 2/10, Loss: 0.8405\n",
      "Epoch 3/10, Loss: 0.8215\n",
      "Epoch 4/10, Loss: 0.8111\n",
      "Epoch 5/10, Loss: 0.8063\n",
      "Epoch 6/10, Loss: 0.7950\n",
      "Epoch 7/10, Loss: 0.7924\n",
      "Epoch 8/10, Loss: 0.7866\n",
      "Epoch 9/10, Loss: 0.7840\n",
      "Epoch 10/10, Loss: 0.7799\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8099\n",
      "Precision: 0.8177\n",
      "Recall: 0.8099\n",
      "F1_score: 0.8113\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.T5(w2v_vectors, labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3t8eqX4WYz-"
   },
   "source": [
    "**FastText - T5 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03Gt9G2PWDFT",
    "outputId": "90fdf1ef-279d-4519-d303-c34f3f7dc879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/mt5-small modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/5, Loss: 0.9114\n",
      "Epoch 2/5, Loss: 0.8619\n",
      "Epoch 3/5, Loss: 0.8417\n",
      "Epoch 4/5, Loss: 0.8316\n",
      "Epoch 5/5, Loss: 0.8241\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7704\n",
      "Precision: 0.7867\n",
      "Recall: 0.7704\n",
      "F1_score: 0.7758\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.T5(ft_vectors, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txxBV3hRWeq1"
   },
   "source": [
    "**TF IDF - T5 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0unUS-JTWEfK",
    "outputId": "49f81e5c-061c-40d8-e047-227d14e83161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/mt5-small modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/5, Loss: 0.7986\n",
      "Epoch 2/5, Loss: 0.6768\n",
      "Epoch 3/5, Loss: 0.6441\n",
      "Epoch 4/5, Loss: 0.6261\n",
      "Epoch 5/5, Loss: 0.6154\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8725\n",
      "Precision: 0.8740\n",
      "Recall: 0.8725\n",
      "F1_score: 0.8731\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.T5(tfidf_vectors, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I03Y9K3UWsjb"
   },
   "source": [
    "**BERT Embedding - T5 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_d4T6n9WEzu",
    "outputId": "90de57e9-eedf-45e5-beeb-50f2329d856c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/mt5-small modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/9, Loss: 0.8383\n",
      "Epoch 2/9, Loss: 0.7933\n",
      "Epoch 3/9, Loss: 0.7788\n",
      "Epoch 4/9, Loss: 0.7714\n",
      "Epoch 5/9, Loss: 0.7664\n",
      "Epoch 6/9, Loss: 0.7607\n",
      "Epoch 7/9, Loss: 0.7591\n",
      "Epoch 8/9, Loss: 0.7522\n",
      "Epoch 9/9, Loss: 0.7505\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.8318\n",
      "Precision: 0.8459\n",
      "Recall: 0.8318\n",
      "F1_score: 0.8343\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.T5(bert_embeddings, labels,epochs=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_deVXifWyRK"
   },
   "source": [
    "**T5 Embedding - T5 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wzUqwrZWFmh",
    "outputId": "d9a6e129-dd32-4b9a-d9d1-b0c3e456ce4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/mt5-small modeli ile sınıflandırma modeli eğitiliyor...\n",
      "Epoch 1/10, Loss: 0.8574\n",
      "Epoch 2/10, Loss: 0.8081\n",
      "Epoch 3/10, Loss: 0.7957\n",
      "Epoch 4/10, Loss: 0.7866\n",
      "Epoch 5/10, Loss: 0.7791\n",
      "Epoch 6/10, Loss: 0.7768\n",
      "Epoch 7/10, Loss: 0.7705\n",
      "Epoch 8/10, Loss: 0.7674\n",
      "Epoch 9/10, Loss: 0.7639\n",
      "Epoch 10/10, Loss: 0.7604\n",
      "Test Sonuçları:\n",
      "Accuracy: 0.7446\n",
      "Precision: 0.8237\n",
      "Recall: 0.7446\n",
      "F1_score: 0.7637\n"
     ]
    }
   ],
   "source": [
    "metrics = tcr.T5(t5_embeddings, labels, epochs=7)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
